{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of lesson3-imdb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daammon/Stock-Prediction-with-RNN-in-Pytorch/blob/master/Copy_of_Copy_of_lesson3_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZwCElNQJ4WY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IMDB"
      ]
    },
    {
      "metadata": {
        "id": "_PqKwDyl4WZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7iv0HYe4WZx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.text import *\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-jpz4lJn4u_3",
        "colab_type": "code",
        "outputId": "72fe7741-abca-4765-9576-9f6c71d57a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('MrTrumpSpeeches.csv', sep='\\~', quoting=3)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>playlist</th>\n",
              "      <th>upload_date</th>\n",
              "      <th>title</th>\n",
              "      <th>view_count</th>\n",
              "      <th>average_rating</th>\n",
              "      <th>like_count</th>\n",
              "      <th>dislike_count</th>\n",
              "      <th>subtitles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2WTNSujhjk</td>\n",
              "      <td>Donald Trump Speeches &amp; Events</td>\n",
              "      <td>20160220</td>\n",
              "      <td>Live Stream: Donald Trump Victory Rally in Spa...</td>\n",
              "      <td>4057.0</td>\n",
              "      <td>4.259259</td>\n",
              "      <td>44.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>presidents of the United States mr. go   tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-64nfy6i58w</td>\n",
              "      <td>Donald Trump Speeches &amp; Events</td>\n",
              "      <td>20161107</td>\n",
              "      <td>LAST RALLY: Donald Trump FINAL CAMPAIGN Rally ...</td>\n",
              "      <td>47276.0</td>\n",
              "      <td>4.358025</td>\n",
              "      <td>952.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>it's now officially Tuesday November a   di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-7Sp31hTxkU</td>\n",
              "      <td>Donald Trump Speeches &amp; Events</td>\n",
              "      <td>20160423</td>\n",
              "      <td>FULL SPEECH: Donald Trump Rally in Bridgeport,...</td>\n",
              "      <td>19966.0</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>220.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>you   [Music]   [Music]   [Music]   you   I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-byuyavcNI4</td>\n",
              "      <td>Donald Trump Speeches &amp; Events</td>\n",
              "      <td>20160617</td>\n",
              "      <td>Full Speech: Donald Trump Rally in Houston, Te...</td>\n",
              "      <td>15138.0</td>\n",
              "      <td>4.582491</td>\n",
              "      <td>266.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>we welcome stars and president   [Music]   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>09BXh-AA72M</td>\n",
              "      <td>Donald Trump Speeches &amp; Events</td>\n",
              "      <td>20161105</td>\n",
              "      <td>Full Speech: Donald Trump Rally in Denver, Col...</td>\n",
              "      <td>8720.0</td>\n",
              "      <td>4.924731</td>\n",
              "      <td>365.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>you   thank you   [Music]   great people Gr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                        playlist  upload_date  \\\n",
              "0  -2WTNSujhjk  Donald Trump Speeches & Events     20160220   \n",
              "1  -64nfy6i58w  Donald Trump Speeches & Events     20161107   \n",
              "2  -7Sp31hTxkU  Donald Trump Speeches & Events     20160423   \n",
              "3  -byuyavcNI4  Donald Trump Speeches & Events     20160617   \n",
              "4  09BXh-AA72M  Donald Trump Speeches & Events     20161105   \n",
              "\n",
              "                                               title  view_count  \\\n",
              "0  Live Stream: Donald Trump Victory Rally in Spa...      4057.0   \n",
              "1  LAST RALLY: Donald Trump FINAL CAMPAIGN Rally ...     47276.0   \n",
              "2  FULL SPEECH: Donald Trump Rally in Bridgeport,...     19966.0   \n",
              "3  Full Speech: Donald Trump Rally in Houston, Te...     15138.0   \n",
              "4  Full Speech: Donald Trump Rally in Denver, Col...      8720.0   \n",
              "\n",
              "   average_rating  like_count  dislike_count  \\\n",
              "0        4.259259        44.0           10.0   \n",
              "1        4.358025       952.0          182.0   \n",
              "2        4.666667       220.0           20.0   \n",
              "3        4.582491       266.0           31.0   \n",
              "4        4.924731       365.0            7.0   \n",
              "\n",
              "                                           subtitles  \n",
              "0     presidents of the United States mr. go   tr...  \n",
              "1     it's now officially Tuesday November a   di...  \n",
              "2     you   [Music]   [Music]   [Music]   you   I...  \n",
              "3     we welcome stars and president   [Music]   ...  \n",
              "4     you   thank you   [Music]   great people Gr...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "97XC2j4HPZtE",
        "colab_type": "code",
        "outputId": "d1f2c894-8559-4e7a-f6a5-ad20c4ebc473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "type(df['subtitles'][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "tfVFk6G9Los1",
        "colab_type": "code",
        "outputId": "72b07248-991c-42b3-b222-86c5c1469c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#for m in pd['subtitles']:\n",
        "#m = re.search('(?<=abc)def', 'abcdef')\n",
        "#m.group(0)\n",
        "m = re.search(r'\\[Music]', df['subtitles'][0])\n",
        "print(m.group(0))\n",
        "new_text = re.sub('[Music]','', df['subtitles'][0])\n",
        "m = re.search(r'\\[Music]', new_text)\n",
        "#m.group(0)\n",
        "print(new_text)  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Music]\n",
            "   predent of the Unted State mr. go   trapp famly   yo   yo   thank yo   [Applae]   []   everybody well I want to begn by   thankng the people of Soth Carolna   th  a peal tate thank yo thee   are peal people and yo know we get a   lttle boot lat week from a plae we   all remember New Hamphre we an't   forget t peal peal we love t and   they ent  n here wth a very good   feelng rght very good feelng o I   really want to thank yo and my   volnteer all of thee people   volnteer and and they travel and they   go I ay what are yo dong and now   they're gong to Texa and they're gong   all over ome are gong to Nevada I'll   be gong to Nevada we're makng a bg   peeh tomorrow n Atlanta and then   we're gong rght to Nevada and I thnk   we're gong to do terrf there and the   SEC  gong to be very very extng we   expet t to very very well I want to   jt ay thank yo to my famly t'   been not eay for them they don't ee me   anymore makng peehe all the tme and   I have a great famly I really have a   great famly o I jt want to thank all   of yo and vanka yo know we have a   hoptal ready jt n ae n Soth   Carolna we're gong to have a baby   there' nothng wrong wth that rght o   old be any t old be any eond that   old even be before I'm fnhed I mean   bt t' bt yo know he nted on   beng here wa o mportant to vanka to   nt on beng o thank really good   Vanea Don we have a wonderfl   letenant governor who baked  very   early n the proe yo know Henry   rght the letenant governor of Soth   Carolna I wll take hm over the   governor any tme beae we want me one   [Applae]   he an handle that very ne he' togh   he' very togh Tffany Erk Lora and   lana lana aa jt want to ay an   amazng plae Soth Carolna   ongratlaton to my hband he wa   workng very hard and he love yo we   love yo and to be gong ahead to Nevada   and we wll ee what happened he wold   be the bet predent   [Applae]   and repreentng ome very very   wonderfl hldren vanka jt a fool   Thank Yo Soth Carolna th  an   amazng amazng nght the momentm ne   the begnnng of th ampagn ha been   nbelevable and that' beae my   father' meage reonate o deeply   wth o many people o a or famly   we're nredbly prod we're nredbly   gratefl to eah of yo o thank yo for   beng here to pport  and I wll ay   th my father  an nredbly hard   worker and he'll be workng for eah and   every one of yo o together we'll make   Amera great agan thank yo very mh   thank yo very mh o yo know  wa   wathng ptar and t wa really   amazng to be wathng what  wa   wathng and ome of the pndt and I   yo know overall far bt not too mh   bt a nmber of the pndt and well f   a ople of the other anddate dropped   ot f yo add ther ore together   t' gong to eqal Trmp rght thee   gene they're gene they don't   ndertand that a people drop ot I'm   gong to get a lot of thoe vote all o   yo don't jt yo're jt add them   together   [Applae]   o I thnk we're gong to do very very   well I thnk we're gong to do very well   I want to alo ongratlate the other   anddate n partlar I have to ay   Ted and aro dd a really good job and   they got they dd qte well a I   ndertand hm and ome on jt of jt   one mnte ome on one eond rght good   okay we go bak to for tomorrow mornng   tomorrow mornng we'll be bak bt I   jt want to ongratlate the other   anddate there' nothng eay abot   rnnng for predent I an tell yo   t' togh t' naty t mean t'   vo t' beatfl when yo wn t'   beatfl and we're gong to tart we   are gong to tart wnnng for or   ontry we're gonna tart women beae   or ontry doe t wn anymore doen't   one we don't wn wth the mltary we   an't beat I we have great mltary   bt we an't be ne a we don't wn on   trade yo look at what Chna  dong to    what Japan doe to  what exo    jt kllng  at the border at the   border and wth trade exo  kllng    aboltely wll do the wall don't   worry we're gong to do the wall we're   gong to do the wall and by the way   who' gong to pay for the wall exo    gong to pay for the world and t'   very eay the other poltan ome   down yo an't get exo to pay for the   wall I ad one hndred perent we have   a ffty eght bllon dollar trade   deft wth mexo the wall  gong to   ae 10 to 12 bllon dollar okay   beleve me they wll pay one of the   offal atally one of the ex   offal from mexo had a new   onferene and he annoned that we wll   not be payng for the wall who doe mr.   Trmp thnk he  they ame to me the   pre thee people look at all thoe   amera bak there the pre ame to me   and they told me that he ad wold not   pay for the wall do yo have a omment I   ad ye the wall jt got 10 feet   taller   t' tre and I love exo I love   exo I love Chna I love many of thee   ontre that rp  off beae we   have leader that are nompetent and   don't know what they're dong I love   thee ontre they great I have   thoand and thoand of Hpan   we're gong to Nevada I lead I lead wth   the Hpan I'm leadng n every poll   wth Hpan they love me I love them   the problem  the problem  that the   leader of thee ontre whether t'   exo or Japan or Vetnam whh  jt   dong a bg nmber now all of a dden   or Chna n partlar that' the bg   one the greatet abe of a ontry that   I thnk I've ever een fnanally Chna   what they've done to   the greatet   ngle theft n the htory of the world   they've taken or job they've taken or   money they've taken everythng we're   brngng or job bak folk we're   brngng or job bak we're gong to   brng them bak and I know how to do t   we've had o many we've had o many   nredble endorement and to me   gettng the greatet bne people to   endore me a very mportant Carl Iahn   endore me o many other have endored   me and we're gong to pt thee great   bne people n harge of trade and   when Chna want to ome and negotate   they're gong to negotate not wth a   poltal hak whh  what they have   now they're gong to negotate wth the   bet bne mnd n the world and we   have them n th ontry   o we're gong to do a lot of beatfl   work we're gong to termnate Obamaare   t' gong to be termnated t' gong   to be over t' gong to be repealed and   t' gong to be replaed and yo're   gong to have mh better health are at   a mh maller ot beae Obamaare f   yo look look at the nreae n yor   Obamaare 25 35 45 even ffty fve   perent t' dead t' not workng we're   gong to go to a plan that' gong to be   o mh better and o mh le   expenve o eond amendment by the way   th  a room love the Seond   Amendment thank yo fella th  a   room love and herhe the Seond   Amendment we are gong to protet or   Seond Amendment   [Applae]   ommon ore  gone we are gettng rd   of ommon ore we're brngng edaton   to a loal level the people n th   ommnty every tme I ee them they   want edaton loally the parent the   teaher they want to do t they don't   want brearat n Wahngton tellng   them how to edate ther hldren we   pend a a naton more for edaton /   ppal than any other naton n the world   not even loe more per ppl eond   plae doen't even ext t' o   dfferent and yet ot of 30 ontre   were ranked nmber 30 yo have Chna yo   have Norway Sweden Denmark and then yo   have nmber 30 the Unted State o we   pend the mot and we're at the bottom   of the heap t' not gong to happen   anymore folk not gong to happen   anymore we are gong to bld or   mltary whh I thank beae a nmber   of the people ame down n a nmber of   the pndt and wathng tonght we love   or mltary   we love or mltary and by the way we   love or pole or pole are terrf   we love or pole they are not beng   treated properly we're gong to bld   or mltary o bg o good o trong o   powerfl that nobody  ever gong to   me wth  folk and we're gong to   by we're gong to by the eqpment   that or general are older that   everybody that' n the note want we're   not byng eqpment that ell beae   they have poltal well beae they   take ampagn and they gve ampagn   ontrbton we are gong to get the   eqpment that they want not the   eqpment that they're told to have by   enator and ongremen n Wahngton   we're gong to have great eqpment and   the reaon  an ay that  I'm   elf-fndng my ampagn I'm not gettng   mllon of dollar from all of thee   I'm not gettng mllon of dollar from   all of thee peal nteret and   lobbyt and donor that one they get   t they lterally do whatever the   poltan want that' not gong to   happen and we're gong to take are of   or vet we love or vet and they're   beng treated terrbly wth takng are   of or veteran o we're now off to   Nevada and t' a great tate we have   great people we have great people n or   naton no matter where we go I went to   oble Alabama 35,000 people we went to   Oklahoma reently twe 20,000 people   20,000 people no matter where we go we   fll p the arena over here the other   day we had a 9,000 we had a 10,000 we   have people the only thng that top   the rowd are the wall beae we   an't get them n we have to end   thoand of people away Tme magazne   Tme magazne lat week dd an   nredble omfort over tory they ad   t' a movement and that' what t    t' an nredble movement wth   nredble people nredble people   [Applae]   []   [Applae]   t' an nredble movement wth   nredble people I tell yo and whether   we go honetly whether we go to Dalla   or whether we go anywhere yo ay yo go   to LA yo go anywhere yo ay or people   are nredble and yo know or theme   what' or theme yo know t rght rght   or theme whh I love maybe the   greatet theme of all tme rght and the   word agan eventally  gong to ome   off we're gong to get rd of the whole   theme beae what' gong to happen   make Amera great agan rght and the   lat two week beae I've gotten to   ee o many I ge mllon when yo   add them all p wth all of thee room   and all of thee peehe and I've met   o many people bt the people are o   nredble that I've been ayng make   Amera great agan and t' gong to be   greater than ever before that' the knd   of potental we have greater than ever   before o o o I want to thank   everybody I love yo all agan Soth   Carolna we wll never forget yo we   wll never forget yo we wll never ever   forget Soth Carolna we wll never   forget or great volnteer we love or   volnteer wll never forget all of the   people that have helped  o mh my   famly and folk let' go let' have a   bg wn n Nevada let' have a bg wn   at the SEC let' pt th thng away and   let' make Amera great agan thank yo   very mh thank yo very mh   yo   []   yo   []   yo   []   yo   []   yo   yo   yo   yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LwaxEw3iLk05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6vWHrHML5fZ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bs=48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hM9qDdG_4vMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = (TextList.from_df(df, cols='subtitles')\n",
        "           #Inputs: all the text files in path\n",
        "            #.filter_by_folder(include=['train', 'test', 'unsup']) \n",
        "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
        "            .random_split_by_pct(0.1)\n",
        "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
        "            .label_for_lm()           \n",
        "           #We want to do a language model so we label accordingly\n",
        "            .databunch(bs=bs))\n",
        "data_lm.save('data_lm.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zed1cPwe4vWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MU3vMVI-4vf6",
        "colab_type": "code",
        "outputId": "b6559126-f9b7-49af-e581-b1a23fd80cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:50 <p><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.011548</td>\n",
              "      <td>3.820578</td>\n",
              "      <td>0.270789</td>\n",
              "      <td>00:50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ylX5DrhT4WaB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "metadata": {
        "id": "W-NClXbD4WaI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First let's download the dataset we are going to study. The [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).\n",
        "\n",
        "We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset."
      ]
    },
    {
      "metadata": {
        "id": "nJdMyyY24WaP",
        "colab_type": "code",
        "outputId": "fa880a55-5a0f-486f-9010-2524823d5579",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/home/ubuntu/notebooks/data/imdb_sample/data_clas_export.pkl'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/export_lm.pkl'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/export.pkl'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/texts.csv'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/data_lm_export.pkl'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/export_clas.pkl'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/models'),\n",
              " PosixPath('/home/ubuntu/notebooks/data/imdb_sample/save_data_clas.pkl')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "iDrRbuTs4Wav",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It only contains one csv file, let's have a look at it."
      ]
    },
    {
      "metadata": {
        "id": "cMvMCdqZ4Wa2",
        "colab_type": "code",
        "outputId": "1d8ecc78-4b8d-4d96-eae9-6e5699a50262",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path/'texts.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "jlgcKu8V4WbO",
        "colab_type": "code",
        "outputId": "64899a5b-51ce-4ced-91da-734733743a9f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['text'][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it\\'s also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "7LSe5URX4Wb0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:"
      ]
    },
    {
      "metadata": {
        "id": "FJTqK1JN4Wb9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = TextDataBunch.from_csv(path, 'texts.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiwJ--yY4Wco",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.\n",
        "\n",
        "Before we delve into the explanations, let's take the time to save the things that were calculated."
      ]
    },
    {
      "metadata": {
        "id": "Dc_N2bQQ4WdK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aeJrqHbr4WeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next time we launch this notebook, we can skip the cell above that took a bit of time (and that will take a lot more when you get to the full dataset) and load those results like this:"
      ]
    },
    {
      "metadata": {
        "id": "T-cBTG0F4Weg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = load_data(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mf3kBQxC4We2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "sGJz_Ykt4We9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
        "\n",
        "- we need to take care of punctuation\n",
        "- some words are contractions of two different words, like isn't or don't\n",
        "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
        "\n",
        "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
      ]
    },
    {
      "metadata": {
        "id": "gFgZXC674WfD",
        "colab_type": "code",
        "outputId": "db652797-eb53-465b-c341-e037e08ba258",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = TextClasDataBunch.from_csv(path, 'texts.csv')\n",
        "data.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n \\n  xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj sydney , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n \\n  xxmaj it 's usually satisfying to watch a film director change his style /</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj this film sat on my xxmaj tivo for weeks before i watched it . i dreaded a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj yorkers . \\n \\n  xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" xxmaj la xxmaj ronde</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first xxup 3d game , or even the first xxunk - up . xxmaj it 's also one of the first stealth games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk gaming experience in general . xxmaj with graphics</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pzgRo6Sc4Wft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
        "- the \"'s\" are grouped together in one token\n",
        "- the contractions are separated like this: \"did\", \"n't\"\n",
        "- content has been cleaned for any HTML symbol and lower cased\n",
        "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
      ]
    },
    {
      "metadata": {
        "id": "aZX9lHSj4Wf8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numericalization"
      ]
    },
    {
      "metadata": {
        "id": "bFi20B8j4WgD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
        "\n",
        "The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string)."
      ]
    },
    {
      "metadata": {
        "id": "8Lgen6ig4WgO",
        "colab_type": "code",
        "outputId": "94b5fefb-d942-4512-e9f6-ee9fb4d2613d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.vocab.itos[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk',\n",
              " 'xxpad',\n",
              " 'xxbos',\n",
              " 'xxfld',\n",
              " 'xxmaj',\n",
              " 'xxup',\n",
              " 'xxrep',\n",
              " 'xxwrep',\n",
              " 'the',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "JhX17vXf4Wgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:"
      ]
    },
    {
      "metadata": {
        "id": "amqph1XO4Wg2",
        "colab_type": "code",
        "outputId": "f833a6a7-2fc5-4355-e71f-7042cf495882",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.train_ds[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos i know that originally , this film was xxup not a box office hit , but in light of recent xxmaj hollywood releases ( most of which have been decidedly formula - ridden , plot less , pointless , \" save - the - blonde - chick - no - matter - what \" xxunk ) , xxmaj xxunk of xxmaj all xxmaj xxunk , certainly in this sorry context deserves a second opinion . xxmaj the film -- like the book -- loses xxunk in some of the historical background , but it xxunk a uniquely xxmaj american dilemma set against the uniquely horrific xxmaj american xxunk of human xxunk , and some of its tragic ( and funny , and touching ) consequences . \n",
              "\n",
              " xxmaj and worthy of xxunk out is the youthful xxmaj robert xxmaj xxunk , cast as the leading figure , xxmaj xxunk , whose xxunk xxunk is truly universal as he sets out in the beginning of his ' coming of age , ' only to be xxunk disappointed at what turns out to become his true education in the ways of the xxmaj southern plantation world of xxmaj xxunk , at the xxunk of the xxunk period . xxmaj when i saw the previews featuring the ( xxunk ) blond - xxunk xxmaj xxunk , i expected a xxunk , a xxunk , a xxunk -- i was pleasantly surprised . \n",
              "\n",
              " xxmaj xxunk xxmaj davis , xxmaj ruby xxmaj dee , the late xxmaj ben xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj victoria xxmaj xxunk and even xxmaj xxunk xxmaj guy xxunk vivid imagery and formidable skill as actors in the backdrop xxunk of xxunk , voodoo , xxmaj xxunk \" xxunk , \" and xxmaj xxunk revolt woven into this tale of human passion , hate , love , family , and racial xxunk in a society which is supposedly gone and yet somehow is still with us ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "c3vNbCDL4Whj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But the underlying data is all numbers"
      ]
    },
    {
      "metadata": {
        "id": "VOFwFhV54Whm",
        "colab_type": "code",
        "outputId": "c304b61a-ded9-45e2-bbb6-b276b19de1ea",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.train_ds[0][0].data[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,   18,  146,   19, 3788,   10,   20,   31,   25,    5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "jem1mFXE4WiW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### With the data block API"
      ]
    },
    {
      "metadata": {
        "id": "X8tFD38v4Wig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.\n",
        "\n",
        "With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the variaous arguments to pass will appear in the step they're revelant, so it'll be more readable."
      ]
    },
    {
      "metadata": {
        "id": "9ED8etza4Wiz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
        "                .split_from_df(col=2)\n",
        "                .label_from_df(cols=0)\n",
        "                .databunch())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdeJyP3n4WjN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Language model"
      ]
    },
    {
      "metadata": {
        "id": "Ce_CNnlq4WjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that language models can use a lot of GPU, so you may need to decrease batchsize here."
      ]
    },
    {
      "metadata": {
        "id": "ZqbM3_AS4Wjm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bs=48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tY_qcqko4Wj5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's grab the full dataset for what follows."
      ]
    },
    {
      "metadata": {
        "id": "EVaX-fxc4Wj-",
        "colab_type": "code",
        "outputId": "108cc60e-cc58-4037-e75d-bff3d99624dd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.IMDB)\n",
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/home/ubuntu/.fastai/data/imdb/test'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/tmp_clas'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/README'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/unsup'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/train'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/tmp_lm'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/models'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/imdb.vocab')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "KoCtXlZK4Wkc",
        "colab_type": "code",
        "outputId": "780a77b9-1681-4618-f77a-389c3d20f6f2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(path/'train').ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/home/ubuntu/.fastai/data/imdb/train/neg'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/train/unsupBow.feat'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/train/pos'),\n",
              " PosixPath('/home/ubuntu/.fastai/data/imdb/train/labeledBow.feat')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "hDG0Tnyh4Wks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.\n",
        "\n",
        "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
        "\n",
        "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on."
      ]
    },
    {
      "metadata": {
        "id": "SG2okxmS4Wkv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes)."
      ]
    },
    {
      "metadata": {
        "id": "tKgZS5fy4Wk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = (TextList.from_folder(path)\n",
        "           #Inputs: all the text files in path\n",
        "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
        "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
        "            .random_split_by_pct(0.1)\n",
        "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
        "            .label_for_lm()           \n",
        "           #We want to do a language model so we label accordingly\n",
        "            .databunch(bs=bs))\n",
        "data_lm.save('data_lm.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EtA0Ks274WlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
        "\n",
        "The line before being a bit long, we want to load quickly the final ids by using the following cell."
      ]
    },
    {
      "metadata": {
        "id": "xKT_fqFV4WlY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YwTH2VZe4Wlh",
        "colab_type": "code",
        "outputId": "d5ac46ab-f3e9-413d-833f-45bf18dfcdfd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>  <col width='5%'>  <col width='95%'>  <tr>\n",
              "    <th>idx</th>\n",
              "    <th>text</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>0</th>\n",
              "    <th>original script that xxmaj david xxmaj dhawan has worked on . xxmaj this one was a complete bit y bit rip off xxmaj hitch . i have nothing against remakes as such , but this one is just so lousy that it makes you even hate the original one ( which was pretty decent ) . i fail to understand what actors like xxmaj salman and xxmaj govinda saw in</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>' classic ' xxmaj the xxmaj big xxmaj doll xxmaj house ' , which takes xxmaj awful to a whole new level . i can heartily recommend these two xxunk as a double - bill . xxmaj you 'll laugh yourself silly . xxbos xxmaj this movie is a pure disaster , the story is stupid and the editing is the worst i have seen , it confuses you incredibly</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>2</th>\n",
              "    <th>of xxmaj european cinema 's most quietly disturbing sociopaths and one of the most memorable finales of all time ( shamelessly stolen by xxmaj tarantino for xxmaj kill xxmaj bill xxmaj volume xxmaj two ) , but it has plenty more to offer than that . xxmaj playing around with chronology and inverting the usual clichs of standard ' lady vanishes ' plots , it also offers superb characterisation and</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>3</th>\n",
              "    <th>but even xxmaj martin xxmaj short managed a distinct , supporting character . ) \\n\\n i can understand the attraction of an imaginary world created in a good romantic comedy . xxmaj but this film is the prozac version of an imaginary world . i 'm frightened to consider that anyone could enjoy it even as pure fantasy . xxbos movie i have ever seen . xxmaj actually i find</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>4</th>\n",
              "    <th>xxmaj pre - xxmaj code film . xxbos xxmaj here 's a decidedly average xxmaj italian post apocalyptic take on the hunting / killing humans for sport theme ala xxmaj the xxmaj most xxmaj dangerous xxmaj game , xxmaj turkey xxmaj shoot , xxmaj gymkata and xxmaj the xxmaj running xxmaj man . \\n\\n xxmaj certainly the film reviewed here is nowhere near as much fun as the other listed</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1MMISPrs4Wly",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file)."
      ]
    },
    {
      "metadata": {
        "id": "63_VaQoM4Wl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dYEx2f2b4Wl7",
        "colab_type": "code",
        "outputId": "eeecb042-f657-4ba0-bd7e-254e1310518f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "otsyvxan4WmU",
        "colab_type": "code",
        "outputId": "fed0ad7d-86e3-41aa-d9f9-b80b02ccc52b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot(skip_end=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1LnLu8i44Wmc",
        "colab_type": "code",
        "outputId": "47fa26cc-f48a-4ab7-f77f-8b8f0ce2e3d9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 16:47 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>4.232034</th>\n",
              "    <th>4.060273</th>\n",
              "    <th>0.292894</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "odWZ4rFW4Wmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('fit_head')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPDfAd4l4Wmt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('fit_head');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYxSMWbe4Wm2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To complete the fine-tuning, we can then unfeeze and launch a new training."
      ]
    },
    {
      "metadata": {
        "id": "2izxrOTg4Wm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P_E8ta7h4WnB",
        "colab_type": "code",
        "outputId": "431320eb-b033-49ee-e554-5f0b239a889c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 10:32 <p><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.712088</td>\n",
              "      <td>3.726514</td>\n",
              "      <td>0.283750</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.567067</td>\n",
              "      <td>3.615324</td>\n",
              "      <td>0.300551</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.382950</td>\n",
              "      <td>3.516003</td>\n",
              "      <td>0.316935</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.167136</td>\n",
              "      <td>3.447631</td>\n",
              "      <td>0.328467</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.940979</td>\n",
              "      <td>3.414895</td>\n",
              "      <td>0.335729</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.757712</td>\n",
              "      <td>3.408578</td>\n",
              "      <td>0.340565</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.612724</td>\n",
              "      <td>3.413921</td>\n",
              "      <td>0.342604</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.502638</td>\n",
              "      <td>3.416965</td>\n",
              "      <td>0.343378</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.386293</td>\n",
              "      <td>3.425055</td>\n",
              "      <td>0.344137</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.361985</td>\n",
              "      <td>3.433965</td>\n",
              "      <td>0.343527</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pAtWefwN4Wna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('fine_tuned')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_in5V7tI4Wnk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How good is our model? Well let's try to see what it predicts after a few given words."
      ]
    },
    {
      "metadata": {
        "id": "MjsaAHWo4Wnq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('fine_tuned');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4DYMaq_y4Wn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT = \"Make America Great again\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iyHIbZoJ4Wn4",
        "colab_type": "code",
        "outputId": "6a24dbc3-92f3-42ca-e7cf-b1be2431a871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make America Great again [ Applause ] thank you thank you very much thank you thank you [ Music ] [ Music ] [ Music ] [ Music ] [ Applause ] [ Music ] you you\n",
            "Make America Great again Thank You Man the American People for Thank You Man oh no Oh beautiful let me tell you mr . Trump thank you for being here today and where are\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y98arz7W4WoH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We not only have to save the model, but also it's encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
      ]
    },
    {
      "metadata": {
        "id": "6Yf_nXGn4WoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save_encoder('fine_tuned_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM33bo2g4WoT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier"
      ]
    },
    {
      "metadata": {
        "id": "IyX6G8xu4WoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
      ]
    },
    {
      "metadata": {
        "id": "-WGtP9yh4WoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FldATW8X4Wog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
        "             #grab all the text files in path\n",
        "             .split_by_folder(valid='test')\n",
        "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
        "             .label_from_folder(classes=['neg', 'pos'])\n",
        "             #label them all with their folders\n",
        "             .databunch(bs=bs))\n",
        "\n",
        "data_clas.save('data_clas.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a7VESAgO4Wol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "31IjKSlm4Wop",
        "colab_type": "code",
        "outputId": "e79fe03c-f0fe-40bc-b079-f4e266827b04",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>  <col width='90%'>  <col width='10%'>  <tr>\n",
              "    <th>text</th>\n",
              "    <th>target</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules</th>\n",
              "    <th>pos</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love</th>\n",
              "    <th>pos</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>xxbos xxmaj here are the matches . . . ( adv . = advantage ) \\n\\n xxmaj the xxmaj warriors ( xxmaj ultimate xxmaj warrior , xxmaj texas xxmaj tornado and xxmaj legion of xxmaj doom ) v xxmaj the xxmaj perfect xxmaj team ( xxmaj mr xxmaj perfect , xxmaj ax , xxmaj smash and xxmaj crush of xxmaj demolition ) : xxmaj ax is the first to go</th>\n",
              "    <th>neg</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>xxbos i felt duty bound to watch the 1983 xxmaj timothy xxmaj dalton / xxmaj zelah xxmaj clarke adaptation of \" xxmaj jane xxmaj eyre , \" because i 'd just written an article about the 2006 xxup bbc \" xxmaj jane xxmaj eyre \" for xxunk . \\n\\n xxmaj so , i approached watching this the way i 'd approach doing homework . \\n\\n i was irritated at first</th>\n",
              "    <th>pos</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>xxbos xxmaj no , this is n't a sequel to the fabulous xxup ova series , but rather a remake of the events that occurred after the death of xxmaj xxunk ( and the disappearance of xxmaj woodchuck ) . xxmaj it is also more accurate to the novels that inspired this wonderful series , which is why characters ( namely xxmaj orson and xxmaj xxunk ) are xxunk ,</th>\n",
              "    <th>pos</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mMovfvF24Woy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then create a model to classify those reviews and load the encoder we saved before."
      ]
    },
    {
      "metadata": {
        "id": "9XZXjOBo4Wo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
        "learn.load_encoder('fine_tuned_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRGGFueD4Wo6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LA6uQePP4Wo_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "alxGK5mU4Wpb",
        "colab_type": "code",
        "outputId": "1ad702bf-7a8f-44b8-8019-06f95bbb5168",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 03:40 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>0.310078</th>\n",
              "    <th>0.197204</th>\n",
              "    <th>0.926960</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oMLmEF4u4Wpi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CBx-sDf54Wpm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('first');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KmK0ItS64Wpw",
        "colab_type": "code",
        "outputId": "597f9563-e503-42f1-e65e-9105f1f7fb33",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 04:03 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>0.255913</th>\n",
              "    <th>0.169186</th>\n",
              "    <th>0.937800</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "b-qQTiRg4WqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('second')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXTH_BzY4WqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('second');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWo5AbFk4WqY",
        "colab_type": "code",
        "outputId": "9ed50434-e2be-4e82-db73-7b3cb4f72077",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 05:42 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>0.223174</th>\n",
              "    <th>0.165679</th>\n",
              "    <th>0.939600</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dh5K_nLw4Wqi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('third')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0x_g6UVW4Wqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('third');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IVNyqDYn4Wq3",
        "colab_type": "code",
        "outputId": "c603407e-8ef9-450b-f867-7ff52a542960",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 15:17 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>0.240424</th>\n",
              "    <th>0.155204</th>\n",
              "    <th>0.943160</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>2</th>\n",
              "    <th>0.217462</th>\n",
              "    <th>0.153421</th>\n",
              "    <th>0.943960</th>\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DmOW1D5H4Wq_",
        "colab_type": "code",
        "outputId": "d708e58a-26a7-44fe-d495-26c9cb0a85a9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.predict(\"I really loved that movie, it was awesome!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category pos, tensor(1), tensor([7.5928e-04, 9.9924e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "metadata": {
        "id": "QDpAHXGU4Wrz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}